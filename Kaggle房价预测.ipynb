{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import hashlib\r\n",
    "import os\r\n",
    "import tarfile\r\n",
    "import zipfile\r\n",
    "import requests"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "维护一个字典DATA_HUB，其将数据集名称的字符串映射到数据集相关的二元组上，这个二元组包含数据集的url和验证文件完整性的sha-1秘钥。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "DATA_HUB = dict()\r\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "用download函数来下载数据集，将数据集缓存在本地目录中，并返回下载的文件名称。如果缓存目录中已经存在次数据集，并且其sha-1与存储在DATA_HUB中的相匹配，我们将使用缓存的文件以避免重复下载。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def download(name, cache_dir=os.path.join('..', 'data')):   #@save\r\n",
    "    \"\"\" 下载一个DATA_HUB中的文件并返回本地文件名 \"\"\"\r\n",
    "    assert name in DATA_HUB, f\"{name}不存在于{DATA_HUB}\"\r\n",
    "    url, sha1_hash = DATA_HUB[name]\r\n",
    "    os.makedirs(cache_dir, exist_ok=True)\r\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\r\n",
    "    if os.path.exists(fname):\r\n",
    "        sha1 = hashlib.sha1()\r\n",
    "        with open(fname, 'rb') as f:\r\n",
    "            while True:\r\n",
    "                data = f.read(1048576)\r\n",
    "                if not data:\r\n",
    "                    break\r\n",
    "                sha1.update(data)\r\n",
    "        if sha1.hexdigest() == sha1_hash:\r\n",
    "            return fname\r\n",
    "    print(f'正在从{url}下载{fname}...')\r\n",
    "    r = requests.get(url, stream=True, verify=True)\r\n",
    "    with open(fname, 'wb') as f:\r\n",
    "        f.write(r.content)\r\n",
    "    return fname\r\n",
    "\r\n",
    "\r\n",
    "def download_extract(name, folder=None):  #@save\r\n",
    "    \"\"\"下载并解压zip/tar文件。\"\"\"\r\n",
    "    fname = download(name)\r\n",
    "    base_dir = os.path.dirname(fname)\r\n",
    "    data_dir, ext = os.path.splitext(fname)\r\n",
    "    if ext == '.zip':\r\n",
    "        fp = zipfile.ZipFile(fname, 'r')\r\n",
    "    elif ext in ('.tar', '.gz'):\r\n",
    "        fp = tarfile.open(fname, 'r')\r\n",
    "    else:\r\n",
    "        assert False, '只有zip/tar文件可以被解压缩。'\r\n",
    "    fp.extractall(base_dir)\r\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir\r\n",
    "\r\n",
    "\r\n",
    "def download_all():  #@save\r\n",
    "    \"\"\"下载DATA_HUB中的所有文件。\"\"\"\r\n",
    "    for name in DATA_HUB:\r\n",
    "        download(name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "竞赛数据分为训练集合测试集，每条记录都包括房屋的属性值和属性，如街道类型、施工年份、屋顶类型、地下室状况等，一些数据也有可能已经丢失，丢失值会被标记为\"NA\"，因此开始训练之前要清理数据。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import torch\r\n",
    "from torch.utils import data\r\n",
    "from torch import nn\r\n",
    "from d2l import torch as d2l\r\n",
    "\r\n",
    "DATA_HUB['kaggle_house_train'] = (DATA_URL + 'kaggle_house_pred_train.csv',\r\n",
    "                                  '585e9cc93e70b39160e7921475f9bcd7d31219ce')\r\n",
    "\r\n",
    "DATA_HUB['kaggle_house_test'] = (DATA_URL + 'kaggle_house_pred_test.csv',\r\n",
    "                                 'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')\r\n",
    "\r\n",
    "train_data = pd.read_csv(download('kaggle_house_train'))\r\n",
    "test_data = pd.read_csv(download('kaggle_house_test'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(train_data.shape)\r\n",
    "print(test_data.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1460, 81)\n",
      "(1459, 80)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "看看前四个和最后两个特征，以及相应标签（房价）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n",
      "0   1          60       RL         65.0       WD        Normal     208500\n",
      "1   2          20       RL         80.0       WD        Normal     181500\n",
      "2   3          60       RL         68.0       WD        Normal     223500\n",
      "3   4          70       RL         60.0       WD       Abnorml     140000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 整合所有特征\r\n",
    "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\r\n",
    "print(all_features.shape)\r\n",
    "print(all_features.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2919, 79)\n",
      "   MSSubClass MSZoning  LotFrontage  LotArea  YrSold SaleType SaleCondition\n",
      "0          60       RL         65.0     8450    2008       WD        Normal\n",
      "1          20       RL         80.0     9600    2007       WD        Normal\n",
      "2          60       RL         68.0    11250    2008       WD        Normal\n",
      "3          70       RL         60.0     9550    2006       WD       Abnorml\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据清洗\r\n",
    "\r\n",
    "先来处理非文本特征\r\n",
    "\r\n",
    "将缺失值替换为相应特征的平均值\r\n",
    "\r\n",
    "为了将所有特征放在一个共同的尺度上，我们通过将特征重新缩放到零均值和单位方差来标准化数据\r\n",
    "\r\n",
    "$$\r\n",
    " x ← \\frac{x-μ}{σ}\r\n",
    "$$\r\n",
    "\r\n",
    "这样特征值就具有了零均值和单位方差"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "numeric_features = all_features.dtypes[\r\n",
    "    all_features.dtypes != 'object'].index  # 将数值特征挑出来\r\n",
    "all_features[numeric_features] = all_features[numeric_features].apply(\r\n",
    "    lambda x: (x - x.mean()) / (x.std()))\r\n",
    "# 在标准化数据后，所有数据意味着消失，因此我们可以将缺失值设为0\r\n",
    "all_features[numeric_features] = all_features[numeric_features].fillna(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来处理文本特征值，用一次独热编码替换。\r\n",
    "\r\n",
    "例如\"MSZoning\"包含值\"RL\"和\"Rm\"，可以创建两个新的特征\"MSZoning_RL\"和\"MSZoning_Rm\"来替换原来的特征标签。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "all_features = pd.get_dummies(all_features, dummy_na=True)  # 将\"NA\"视为有效特征值，并为其创建指示符特征\r\n",
    "all_features.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2919, 331)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "最后从pandas格式提取Numpy格式，并将其转换为张量进行训练"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "n_train = train_data.shape[0]\r\n",
    "train_features = torch.tensor(all_features[:n_train].values,\r\n",
    "                              dtype=torch.float32)\r\n",
    "test_features = torch.tensor(all_features[n_train:].values,\r\n",
    "                             dtype=torch.float32)\r\n",
    "train_labels = torch.tensor(train_data.SalePrice.values.reshape(-1, 1),\r\n",
    "                            dtype=torch.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 训练"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "loss = nn.MSELoss()\r\n",
    "in_features = train_features.shape[1]   # 331\r\n",
    "\r\n",
    "def get_net():\r\n",
    "    net = nn.Sequential(nn.Linear(in_features,1))\r\n",
    "    return net"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "对于房价，我们关心的是相对数量而不是绝对数量，因此我们更关心相对误差$$\\frac{y-\\hat{y}}{y}$$\r\n",
    "\r\n",
    "因此loss使用均方根误差\r\n",
    "$$\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\log y_i -\\log \\hat{y}_i\\right)^2}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def log_rmse(net, features, labels):\r\n",
    "    # 为了在取对数时进一步稳定该值，将小于1的值设为1\r\n",
    "    clipped_preds = torch.clamp(net(features), 1, float('inf'))     # 将值束缚在[1,inf]之间，小于1的设为1，大于inf的设为inf\r\n",
    "    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))\r\n",
    "    return rmse.item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\r\n",
    "      \"\"\" 构造一个基于Pytorch的数据迭代器 \"\"\"\r\n",
    "      dataset = data.TensorDataset(*data_arrays)  # 表明datat_attays是一个元组\r\n",
    "      return data.DataLoader(dataset, batch_size, shuffle=is_train)  # is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据\r\n",
    "\r\n",
    "\r\n",
    "def train(net, train_features, train_labels, test_features, test_labels,\r\n",
    "      num_epochs, learning_rate, weight_decay, batch_size):\r\n",
    "      train_ls, test_ls = [], []\r\n",
    "      train_iter = load_array((train_features, train_labels), batch_size)\r\n",
    "      optimizer = torch.optim.Adam(net.parameters(),\r\n",
    "                                    lr=learning_rate,\r\n",
    "                                    weight_decay=weight_decay)  # 使用Adam优化算法\r\n",
    "      for epoch in range(num_epochs):\r\n",
    "            for X, y in train_iter:\r\n",
    "                  optimizer.zero_grad()\r\n",
    "                  l = loss(net(X), y)\r\n",
    "                  l.backward()\r\n",
    "                  optimizer.step()\r\n",
    "            train_ls.append(log_rmse(net, train_features, train_labels))\r\n",
    "            if test_ls is not None:\r\n",
    "                  test_ls.append(log_rmse(net, test_features, test_labels))\r\n",
    "      return train_ls, test_ls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K折交叉验证\r\n",
    "\r\n",
    "在K折交叉验证时返回第i折的数据，选择第i个切片作为验证数据，其余部分作为训练数据。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def get_k_fold_data(k, i, X, y):\r\n",
    "    \"\"\" 当返回到第i折时，返回训练用数据和验证用的数据 \"\"\"\r\n",
    "    assert k > 1\r\n",
    "    fold_size = X.shape[0] // k  # 返回商的整数部分，向下取整\r\n",
    "    X_train, y_train = None, None\r\n",
    "    for j in range(k):\r\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\r\n",
    "        X_part, y_part = X[idx, :], y[idx]\r\n",
    "        if j == i:\r\n",
    "            X_valid, y_valid = X_part, y_part\r\n",
    "        elif X_train is None:\r\n",
    "            X_train, y_train = X_part, y_part\r\n",
    "        else:\r\n",
    "            X_train = torch.cat([X_train, X_part], 0)\r\n",
    "            y_train = torch.cat([y_train, y_part], 0)\r\n",
    "    return X_train, y_train, X_valid, y_valid"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,\r\n",
    "            batch_size):\r\n",
    "      \"\"\" 在交叉验证中训练k次后返回训练和验证的平均误差 \"\"\"\r\n",
    "      train_l_sum, valid_l_sum = 0, 0\r\n",
    "      for i in range(k):\r\n",
    "            data = get_k_fold_data(k, i, X_train, y_train)\r\n",
    "            net = get_net()\r\n",
    "            train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\r\n",
    "                                          weight_decay, batch_size)\r\n",
    "            train_l_sum += train_ls[-1]\r\n",
    "            valid_l_sum += valid_ls[-1]\r\n",
    "            print(f'fold {i + 1}, train log rmse {float(train_ls[-1]):f}, '\r\n",
    "                  f'valid log rmse {float(valid_ls[-1]):f}')\r\n",
    "      return train_l_sum / k, valid_l_sum / k"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "k = 5\r\n",
    "num_epochs = 100\r\n",
    "lr = 5\r\n",
    "weight_decay = 0\r\n",
    "batch_size = 64\r\n",
    "train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,\r\n",
    "                            weight_decay, batch_size)\r\n",
    "print(f'{k}-折验证: 平均训练log rmse: {float(train_l):f}, '\r\n",
    "        f'平均验证log rmse: {float(valid_l):f}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gg\n",
      "fold 1, train log rmse 0.169715, valid log rmse 0.156277\n",
      "5-折验证: 平均训练log rmse: 0.033943, 平均验证log rmse: 0.031255\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit"
  },
  "interpreter": {
   "hash": "1afac9f31d94eb4bb33e3dfb678ca317679229fb6cdf46f98be1090e60a0b866"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}