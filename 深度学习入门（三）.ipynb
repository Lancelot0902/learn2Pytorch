{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "线性回归的简洁实现"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import torch\r\n",
    "import d2l\r\n",
    "import numpy as np\r\n",
    "from torch.utils import data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "生成数据集"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "true_w = torch.tensor([2, -3.4])\r\n",
    "true_b = 4.2\r\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "调用API读取数据集"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):     #@save\r\n",
    "    \"\"\" 构造一个基于Pytorch的数据迭代器 \"\"\"\r\n",
    "    dataset = data.TensorDataset(*data_arrays)  # 表明datat_attays是一个元组\r\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)   # is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据\r\n",
    "\r\n",
    "batch_size = 10\r\n",
    "data_iter = load_array((features, labels), batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "验证数据迭代器是否正常工作"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "next(iter(data_iter))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor([[ 0.2348, -0.6778],\n",
       "         [ 1.0357, -0.1975],\n",
       "         [-1.1187, -1.4215],\n",
       "         [ 0.0192, -1.5897],\n",
       "         [ 0.1269,  0.0876],\n",
       "         [ 2.5652, -0.4655],\n",
       "         [-0.6237,  0.0599],\n",
       "         [ 0.0806, -1.0334],\n",
       "         [ 0.5170,  2.3261],\n",
       "         [ 0.7248, -1.0568]]),\n",
       " tensor([[ 6.9658],\n",
       "         [ 6.9335],\n",
       "         [ 6.7877],\n",
       "         [ 9.6526],\n",
       "         [ 4.1684],\n",
       "         [10.9116],\n",
       "         [ 2.7545],\n",
       "         [ 7.8775],\n",
       "         [-2.6872],\n",
       "         [ 9.2379]])]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义模型，首先定义一个模型变量net，它是一个Sequential类的实例，Sequential类为串联在一起的多个层定义了一个容器。当给定输入数据，Sequential类实例将数据传入第一层，然后第一层的输出作为第二层的输入，以此类推。\r\n",
    "\r\n",
    "我们目前的线性回归模型是只有一层的网络架构，这一单层被称为**全连接层**，因为它的每一个输入都是通过矩阵-向量乘法连接到它的每个输出。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Pytorch中，全连接层在Linear类中定义\r\n",
    "# 将两个参数传递到nn.Linear中，第一个为输入的特征形状，第二个为输出的特征形状\r\n",
    "\r\n",
    "from torch import nn    # nn是神经网络neural network的缩写\r\n",
    "\r\n",
    "net = nn.Sequential(nn.Linear(2, 1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "初始化模型参数。\r\n",
    "\r\n",
    "net[0]表示选择网络中的第一个图层，即我们的线性全连接层，然后定义权重weight和偏置bias的值。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "net[0].weight.data.normal_(0, 0.01)\r\n",
    "net[0].bias.data.fill_(0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义损失函数。\r\n",
    "\r\n",
    "使用的是MSELoss类，默认情况下会返回所有样本损失的均值。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "loss = nn.MSELoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义优化算法。\r\n",
    "\r\n",
    "Pytorch在optim模块实现了该算法的许多变种。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# 需要优化的参数通过net.parameters()中获得\r\n",
    "# 小批量随机梯度下降只需要设置lr的值\r\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "num_epochs = 3\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    for X, y in data_iter:\r\n",
    "        l = loss(net(X), y)\r\n",
    "        trainer.zero_grad()\r\n",
    "        l.backward()\r\n",
    "        trainer.step()\r\n",
    "    l = loss(net(features), labels)\r\n",
    "    print(f'epoch {epoch + 1},loss {l:f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1,loss 0.000098\n",
      "epoch 2,loss 0.000097\n",
      "epoch 3,loss 0.000097\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "w = net[0].weight.data\r\n",
    "print('w的估计误差: ', true_w - w.reshape(true_w.shape))\r\n",
    "b = net[0].bias.data\r\n",
    "print('b的估计误差: ', true_b - b)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w的估计误差:  tensor([-0.0003, -0.0007])\n",
      "b的估计误差:  tensor([-0.0002])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit"
  },
  "interpreter": {
   "hash": "1afac9f31d94eb4bb33e3dfb678ca317679229fb6cdf46f98be1090e60a0b866"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}